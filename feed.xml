<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://xqsrd666.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xqsrd666.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-22T05:32:20+00:00</updated><id>https://xqsrd666.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">YOLO目标检测系列解析——从YOLOv1到YOLOv11</title><link href="https://xqsrd666.github.io/blog/2025/yolo/" rel="alternate" type="text/html" title="YOLO目标检测系列解析——从YOLOv1到YOLOv11"/><published>2025-09-21T14:30:00+00:00</published><updated>2025-09-21T14:30:00+00:00</updated><id>https://xqsrd666.github.io/blog/2025/yolo</id><content type="html" xml:base="https://xqsrd666.github.io/blog/2025/yolo/"><![CDATA[<p>在目标检测的发展脉络中，YOLO（You Only Look Once）系列代表了一类典型的<strong>单阶段检测器</strong>，通过将目标检测任务视为回归问题，将图像划分网格并直接预测每个区域的边界框和类别概率，实现了高效端到端检测。</p> <p>YOLO的出现旨在解决传统两阶段算法推理速度慢、复杂度高的问题，同时在精度和速度上取得平衡。</p> <hr/> <h4 id="-yolo基础版"><strong>📌 YOLO（基础版）</strong></h4> <ul> <li>网络结构：24 个卷积层 + 2 个全连接层</li> <li>核心思想：将图像划分为 k×k 网格，每个网格预测 2 个 bounding boxes、对象置信度和类别概率</li> <li>推理流程： <ol> <li>对图像进行 k×k 网格划分</li> <li>每个网格预测 bounding boxes + 置信度 + 类别概率</li> <li>剔除低置信度框，使用 NMS 去除冗余</li> </ol> </li> <li>技术要点： <ul> <li>激活函数：Leaky ReLU</li> <li>Dropout 正则化：第一个全连接层后</li> <li>损失函数：边界框有无预测误差 + 边界框精度误差 + 分类预测误差</li> </ul> </li> <li><strong>缺陷</strong>： <ul> <li>新物体识别困难</li> <li>大小框误差计算相同 → 精度不高</li> <li>每个网格只预测两个框和一个类别 → 小目标或成群目标检测困难</li> </ul> </li> </ul> <hr/> <h4 id="-yolov2"><strong>📌 YOLOv2</strong></h4> <ul> <li>使用 WordTree 结构，增强对未见物体的泛化能力</li> <li>批量归一化 + 锚框/滑动窗口</li> <li>Darknet-19 作为 backbone 提升速度和精度</li> <li>分层分类框架设计</li> </ul> <hr/> <h4 id="-yolov3"><strong>📌 YOLOv3</strong></h4> <ul> <li>多尺度预测，适应不同尺寸目标</li> <li>Backbone：Darknet-53 = Darknet-19 + ResNet <ul> <li>提升特征表达能力，比 ResNet101 更高效</li> </ul> </li> <li>独立逻辑分类器代替 softmax → 每个盒子可预测多个标签</li> <li>数据集规模小，但获得更高检测精度</li> </ul> <hr/> <h4 id="-yolov4"><strong>📌 YOLOv4</strong></h4> <ul> <li><strong>架构设计</strong>：backbone + neck + head <ol> <li><strong>Backbone</strong> <ul> <li>跨层学习特征的 CNN</li> <li>CSPNet（Cross Stage Partial）策略的 Darknet-53</li> </ul> </li> <li><strong>Neck</strong> <ul> <li>特征整合与细化</li> <li>SPP（空间金字塔池化） + PAN（路径聚合网络）</li> <li>提高特征提取精度和训练效果</li> </ul> </li> <li><strong>Head</strong> <ul> <li>执行预测</li> <li>输出边界框、类别概率和对象性分数</li> <li>基于锚点的 YOLOv3 预测头</li> </ul> </li> </ol> </li> </ul> <hr/> <blockquote> <p>YOLO系列的发展展示了单阶段目标检测从基础回归框架到多尺度、高效网络设计的演进过程。随着YOLOv4及后续版本出现，网络架构逐渐模块化、特征利用更加精细，为实时目标检测任务提供了高精度和高速度的解决方案。</p> </blockquote> <div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;"> <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p> <ul style="list-style:none; padding-left:0;"> <li>⬅️ 上一篇：<a href="/blog/2025/ssd">目标检测——SSD单阶段检测器解析</a></li> </ul> </div>]]></content><author><name></name></author><category term="研究笔记"/><category term="计算机视觉"/><category term="目标检测"/><category term="YOLO"/><category term="Single Shot Detector"/><category term="Darknet"/><category term="多尺度检测"/><summary type="html"><![CDATA[YOLO目标检测算法介绍，包括基础YOLO及YOLOv2、YOLOv3、YOLOv4的网络结构、技术要点和优势缺点分析。]]></summary></entry><entry><title type="html">目标检测——SSD单阶段检测器解析</title><link href="https://xqsrd666.github.io/blog/2025/ssd/" rel="alternate" type="text/html" title="目标检测——SSD单阶段检测器解析"/><published>2025-09-21T13:30:00+00:00</published><updated>2025-09-21T13:30:00+00:00</updated><id>https://xqsrd666.github.io/blog/2025/ssd</id><content type="html" xml:base="https://xqsrd666.github.io/blog/2025/ssd/"><![CDATA[<p>在目标检测的发展脉络中，<strong>One-Stage算法</strong>的出现主要为了解决两阶段方法计算复杂、速度较慢的问题。<br/> 其中 SSD（Single Shot MultiBox Detector）是经典代表之一，它通过<strong>多尺度特征图直接预测目标类别和位置</strong>，实现单阶段端到端检测，兼顾精度与速度。</p> <hr/> <h4 id="-ssdsingle-shot-multibox-detector"><strong>📌 SSD（Single Shot MultiBox Detector）</strong></h4> <p>SSD 的核心思想是在不同尺度的特征图上直接进行目标检测，无需候选框生成和复杂的 ROI Pooling。</p> <hr/> <h4 id="网络结构与推理流程"><strong>网络结构与推理流程</strong></h4> <ol> <li><strong>卷积层</strong> <ul> <li>基础网络通常使用 VGG16</li> <li>提取全图特征，为后续目标定位和分类提供语义信息</li> </ul> </li> <li><strong>目标检测层</strong> <ul> <li>由 5 个卷积层 + 1 个平均池化层组成</li> <li>在每个特征图上生成不同尺度、比例的 <strong>Default Box</strong></li> <li>通过卷积预测每个候选框的： <ul> <li><strong>类别置信度</strong>（Classification）</li> <li><strong>边界框偏移量</strong>（Localization）</li> </ul> </li> </ul> </li> </ol> <div style="text-align:center"> <img src="/assets/img/blogs/detection/ssd-architecture.jpeg" alt="SSD结构" style="max-width:85%; height:auto;"/> </div> <ul> <li>多尺度特征图检测 → 提高小、中、大目标的检测能力</li> <li>输出经过 NMS 筛选后，得到类别标签、置信度分数和精确边界框坐标</li> </ul> <ol> <li><strong>筛选层</strong> <ul> <li>过滤低概率候选框</li> <li>使用 NMS（非极大值抑制）去除高度重叠的框</li> </ul> </li> </ol> <hr/> <h4 id="训练流程"><strong>训练流程</strong></h4> <ol> <li><strong>构造数据集</strong> → 标注边界框</li> <li><strong>前向传播</strong> → 网络输出 Default Box 的类别和位置预测</li> <li><strong>正负样本定义</strong> <ul> <li>与真实框 IoU 高于阈值 → 正样本</li> <li>其余 Default Box → 负样本</li> </ul> </li> <li><strong>损失函数</strong> <ul> <li><strong>定位损失</strong>（Smooth L1 Loss，仅对正样本）</li> <li><strong>置信度损失</strong>（Softmax Loss，对所有样本）</li> <li><strong>难例挖掘</strong>：只选负样本中损失最大的部分，用于缓解正负样本极度不平衡问题</li> </ul> </li> <li><strong>反向传播</strong> → 更新网络权重</li> </ol> <hr/> <h4 id="特点与贡献"><strong>特点与贡献</strong></h4> <ul> <li>使用类似 RPN 的 <strong>多尺度锚点机制</strong> → 提高候选框多样性</li> <li><strong>全卷积网络结构</strong> → 检测速度快</li> <li><strong>多尺度特征图</strong> → 提升不同大小目标检测能力</li> <li>单阶段端到端预测 → 舍弃 RPN，提高简洁性</li> </ul> <hr/> <h4 id="优缺点"><strong>优缺点</strong></h4> <ul> <li><strong>优点</strong>： <ul> <li>推理速度快</li> <li>检测精度高</li> <li>框架设计简单，易于部署</li> </ul> </li> <li><strong>缺点</strong>： <ul> <li>浅层特征语义不足 → 小目标检测效果欠佳</li> <li>正负样本不平衡</li> <li>Default Box 尺寸和比例需要手动微调（超参数敏感）</li> </ul> </li> </ul> <hr/> <blockquote> <p>SSD 作为经典一阶段检测器，通过多尺度特征图直接预测候选框和类别，实现了“两全其美”：在保持较高检测精度的同时大幅提升了推理速度，为实时目标检测奠定基础。</p> </blockquote> <div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;"> <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p> <ul style="list-style:none; padding-left:0;"> <li>➡️ 上一篇：<a href="/blog/2025/ssd">多模态视觉大模型解析——SAM</a></li> <li>⬅️ 下一篇：<a href="/blog/2025/yolo">YOLO目标检测系列解析——从YOLOv1到YOLOv11</a></li> </ul> </div>]]></content><author><name></name></author><category term="研究笔记"/><category term="计算机视觉"/><category term="目标检测"/><category term="SSD"/><category term="Single Shot MultiBox Detector"/><category term="多尺度检测"/><category term="一阶段检测"/><summary type="html"><![CDATA[SSD（Single Shot MultiBox Detector）单阶段目标检测器详细解析，包括网络结构、推理流程、训练策略与优势劣势分析。]]></summary></entry><entry><title type="html">目标检测Two-Stage算法解析</title><link href="https://xqsrd666.github.io/blog/2025/twostage/" rel="alternate" type="text/html" title="目标检测Two-Stage算法解析"/><published>2025-09-21T12:30:00+00:00</published><updated>2025-09-21T12:30:00+00:00</updated><id>https://xqsrd666.github.io/blog/2025/twostage</id><content type="html" xml:base="https://xqsrd666.github.io/blog/2025/twostage/"><![CDATA[<p>在目标检测的发展脉络中，<strong>Two-Stage算法</strong>起到了奠基作用。<br/> 它的核心思想是<strong>先生成候选区域（Region Proposal）</strong>，再对候选区域进行分类与边框回归。相比传统滑动窗口和选择性搜索方法，Two-Stage算法在精度上有明显提升，但也引入了计算复杂度和处理流程复杂的问题。</p> <p>接下来，我们详细梳理这一类方法的发展历程，从 R-CNN 到 R-FCN。</p> <hr/> <h4 id="-r-cnnregions-with-cnn-features"><strong>📌 R-CNN（Regions with CNN features）</strong></h4> <ul> <li><strong>步骤</strong>： <ol> <li>使用 <strong>选择性搜索</strong>生成候选区域（ROI）</li> <li>对每个候选区域裁剪、缩放后送入 CNN 提取特征</li> <li>使用 SVM 分类器判断目标类别</li> </ol> </li> <li><strong>问题</strong>： <ul> <li>候选框数量多且存在重叠 → 计算量大</li> <li>各步骤独立 → 测试繁杂，效率低</li> </ul> </li> </ul> <blockquote> <p>R-CNN虽然开创了候选区域+CNN特征提取的先例，但计算瓶颈明显。</p> </blockquote> <hr/> <h4 id="-spp-netspatial-pyramid-pooling-network"><strong>📌 SPP-Net（Spatial Pyramid Pooling Network）</strong></h4> <ul> <li><strong>目标</strong>：解决 CNN 全连接层对输入尺寸固定的限制</li> <li><strong>核心改进</strong>： <ul> <li>使用 <strong>SPP（空间金字塔池化）</strong> 对卷积特征图进行多尺度池化</li> <li>生成固定长度特征向量，无需对每个候选框进行裁剪缩放</li> </ul> </li> </ul> <div style="text-align:center"> <img src="/assets/img/blogs/twostage/spp_net.jpeg" alt="SPP-Net流程" style="max-width:85%; height:auto;"/> </div> <ul> <li><strong>处理流程</strong>： <ol> <li>全图输入 CNN 提取特征</li> <li>在特征图中定位每个候选框</li> <li>SPP 池化 → 分类与边框回归</li> </ol> </li> </ul> <blockquote> <p>SPP-Net极大提升了计算效率和灵活性。</p> </blockquote> <hr/> <h4 id="-fast-r-cnn"><strong>📌 Fast R-CNN</strong></h4> <ul> <li><strong>改进点</strong>： <ul> <li>引入 <strong>ROI Pooling</strong>，将候选区域池化到固定尺寸 h×w</li> <li>统一 <strong>多任务损失函数</strong>，同时优化分类与边框回归</li> </ul> </li> <li><strong>处理流程</strong>： <ol> <li>全图特征提取</li> <li>将候选框映射到特征图 → ROI Pooling</li> <li>全连接层 → Softmax 分类 + 边框回归</li> </ol> </li> </ul> <blockquote> <p>ROI Pooling 解决了 R-CNN 对每个候选框单独 CNN 的冗余问题。</p> </blockquote> <hr/> <h4 id="-faster-r-cnn"><strong>📌 Faster R-CNN</strong></h4> <ul> <li><strong>核心创新</strong>：提出 <strong>RPN（Region Proposal Network）</strong> 生成候选框</li> <li><strong>处理步骤</strong>： <ol> <li><strong>卷积层</strong>：多层卷积提取全图特征</li> <li><strong>RPN层</strong>： <ul> <li>对每个 anchor 判断是否含物体（二分类）</li> <li>用 <strong>bounding box regression</strong> 调整位置生成 proposals</li> </ul> </li> <li><strong>ROI Pooling</strong>：调整特征维度</li> <li><strong>分类层</strong>：分类 + 回归分支，选择概率最高的类别，再进行精细回归</li> </ol> </li> </ul> <blockquote> <p>Faster R-CNN 实现了候选区域生成和检测器的端到端融合，大幅提升效率。</p> </blockquote> <hr/> <h4 id="-r-fcnregion-based-fully-convolutional-network"><strong>📌 R-FCN（Region-based Fully Convolutional Network）</strong></h4> <ul> <li><strong>背景</strong>：传统分类网络不关注目标精确位置，检测任务需要位置敏感</li> <li> <p><strong>创新点</strong>：<strong>Position-Sensitive Score Maps</strong></p> </li> <li><strong>处理流程</strong>： <ol> <li>特征提取：ResNet-101 卷积层 + 全连接层</li> <li>通道变换：卷积生成 k²*(C+1) 通道（k×k 网格 + 类别 + 背景）</li> <li>ROI划分：候选框划分成 k×k 网格 → 对应通道池化 → 得分矩阵</li> <li>得分聚合：对每类的 k×k 得分矩阵求平均 → 判定类别</li> </ol> </li> </ul> <blockquote> <p>R-FCN 保留全卷积特征提取效率，同时对目标位置敏感度高，兼顾速度与精度。</p> </blockquote> <hr/> <blockquote> <p><strong>总结</strong>：Two-Stage目标检测方法从 R-CNN 开始，经历 SPP-Net、Fast R-CNN、Faster R-CNN 到 R-FCN 的演进，逐步解决了候选框生成、CNN特征提取和 ROI 池化的问题，实现了<strong>更高精度、更高效率和位置敏感性的目标检测</strong>。</p> </blockquote> <hr/> <div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;"> <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p> <ul style="list-style:none; padding-left:0;"> <li>⬅️ 上一篇：<a href="/blog/2025/detection-intro">目标检测——SSD单阶段检测器解析</a></li> <li>➡️ 下一篇：<a href="/blog/2025/ssd">多模态视觉大模型解析——SAM</a></li> </ul> </div>]]></content><author><name></name></author><category term="研究笔记"/><category term="计算机视觉"/><category term="目标检测"/><category term="R-CNN"/><category term="SPP-Net"/><category term="Fast R-CNN"/><category term="Faster R-CNN"/><category term="R-FCN"/><summary type="html"><![CDATA[Two-Stage目标检测算法详细解析，包括R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN及R-FCN的网络结构、处理流程与优化策略。]]></summary></entry><entry><title type="html">目标检测基础——Two-Stage与One-Stage方法</title><link href="https://xqsrd666.github.io/blog/2025/detection-intro/" rel="alternate" type="text/html" title="目标检测基础——Two-Stage与One-Stage方法"/><published>2025-01-03T12:00:00+00:00</published><updated>2025-01-03T12:00:00+00:00</updated><id>https://xqsrd666.github.io/blog/2025/detection-intro</id><content type="html" xml:base="https://xqsrd666.github.io/blog/2025/detection-intro/"><![CDATA[<p>在计算机视觉中，<strong>目标检测</strong>任务的核心是同时完成两个目标：</p> <ol> <li><strong>物体分类</strong>：识别图像中每个目标的类别</li> <li><strong>位置定位</strong>：确定物体在图像中的准确位置（通常以边界框表示）</li> </ol> <hr/> <h4 id="-目标检测算法分类"><strong>📌 目标检测算法分类</strong></h4> <p>根据检测流程，目标检测算法主要分为两类：</p> <ol> <li><strong>Two-Stage（两阶段）算法</strong> <ul> <li>先生成可能存在物体的候选区域（Region Proposal）</li> <li>再对候选区域进行分类与边框回归</li> <li>典型算法：R-CNN、Fast R-CNN、Faster R-CNN</li> </ul> </li> <li><strong>One-Stage（单阶段）算法</strong> <ul> <li>直接在图像上输出最终检测结果</li> <li>更快但精度可能略低</li> <li>典型算法：YOLO、SSD</li> </ul> </li> </ol> <hr/> <h4 id="-目标检测的三个核心模块"><strong>📌 目标检测的三个核心模块</strong></h4> <ol> <li><strong>检测窗口的选择（候选区域生成）</strong></li> <li><strong>图像特征提取</strong></li> <li><strong>分类器设计与边框回归</strong></li> </ol> <hr/> <h4 id="-检测窗口的选择方法"><strong>📌 检测窗口的选择方法</strong></h4> <ol> <li><strong>滑动窗口法（Sliding Window）</strong> <ul> <li>在图像上按照不同大小、间隔滑动窗口</li> <li>利用训练好的分类器筛选高概率区域</li> <li>通过 NMS（非极大值抑制）得到最终检测结果</li> </ul> </li> </ol> <div style="text-align:center"> <img src="/assets/img/blogs/detection-intro/sliding_window.jpeg" alt="滑动窗口法" style="max-width:85%; height:auto;"/> </div> <ol> <li><strong>选择性搜索（Selective Search）</strong> <ul> <li>为提高效率，不盲目搜索整张图像</li> <li>流程：图像分割 → 基于相似度的合并 → 构建外接矩形框</li> <li>最终生成一组感兴趣区域（ROI）</li> </ul> </li> </ol> <div style="text-align:center"> <img src="/assets/img/blogs/detection-intro/selective_search.jpeg" alt="选择性搜索" style="max-width:85%; height:auto;"/> </div> <ol> <li><strong>区域提议网络（RPN, Region Proposal Network）</strong> <ul> <li>通过卷积获得图像特征图</li> <li>在每个特征图像素点创建多个候选框（anchor），不同尺度和纵横比</li> <li>映射回原图即为候选窗口</li> <li>利用卷积特征的语义化信息决定候选框位置与存在概率</li> <li>相比滑动窗口和选择性搜索，RPN更高效、精确</li> </ul> <p><strong>🎛️ RPN网络架构</strong></p> <ul> <li>基于共享卷积特征图，RPN分支为两个子任务： <ol> <li><strong>分类分支</strong>：判断候选框是否包含目标</li> <li><strong>回归分支</strong>：微调候选框位置以更贴合目标边界</li> </ol> </li> <li>优势：</li> <li>高层语义特征驱动的候选区域生成</li> <li>高效替代传统手工设计的滑动窗口和选择性搜索</li> <li>与 Fast/Faster R-CNN 等 Two-Stage 检测器完美集成</li> </ul> </li> </ol> <hr/> <blockquote> <p><strong>总结</strong>：目标检测的核心是“选区域+提特征+分类回归”。Two-Stage算法重视精度，One-Stage算法追求速度。随着深度学习发展，RPN和端到端的检测器（如Faster R-CNN、YOLO系列）大幅提升了检测效率和精度，为后续多模态与分割任务奠定基础。</p> </blockquote> <hr/> <div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;"> <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p> <ul style="list-style:none; padding-left:0;"> <li>➡️ 下一篇：<a href="/blog/2024/clip">多模态视觉大模型解析——CLIP</a></li> <li>➡️ 下一篇：<a href="/blog/2024/twostage">目标检测Two-Stage算法解析</a></li> </ul> </div>]]></content><author><name></name></author><category term="研究笔记"/><category term="计算机视觉"/><category term="目标检测"/><category term="R-CNN"/><category term="YOLO"/><category term="RPN"/><category term="SSD"/><summary type="html"><![CDATA[目标检测算法概述，包括检测任务引入、Two-Stage与One-Stage方法、滑动窗口、选择性搜索和RPN的基本原理与网络结构。]]></summary></entry><entry><title type="html">多模态视觉大模型解析——SAM</title><link href="https://xqsrd666.github.io/blog/2024/sam/" rel="alternate" type="text/html" title="多模态视觉大模型解析——SAM"/><published>2024-12-15T11:00:00+00:00</published><updated>2024-12-15T11:00:00+00:00</updated><id>https://xqsrd666.github.io/blog/2024/sam</id><content type="html" xml:base="https://xqsrd666.github.io/blog/2024/sam/"><![CDATA[<p>在前几篇文章中，我们介绍了 <strong>基于 Transformer 的语义分割模型（SETR、Segmenter、SegFormer、MaskFormer）</strong>。<br/> 随着模型能力提升和大规模数据可用，研究逐渐从固定任务分割向 <strong>零样本、可交互、多模态分割</strong> 发展。<br/> SAM（Segment Anything Model）即是这一趋势的代表，具备 <strong>零样本泛化能力</strong>，通过灵活的提示（点、框、文字）即可分割特定对象。</p> <hr/> <h4 id="-samsegment-anything-model概览"><strong>📌 SAM（Segment Anything Model）概览</strong></h4> <div style="text-align:center"> <img src="/assets/img/blogs/sam/sam.png" alt="SAM模型" style="max-width:85%; height:auto;"/> </div> <ul> <li><strong>核心目标</strong>：通过用户提示从图像中分割特定对象</li> <li><strong>零样本泛化</strong>：无需任务特定标注即可进行分割</li> <li><strong>训练数据</strong>：基于 SA-1B 大规模图像分割数据集</li> <li><strong>主要模块</strong>： <ul> <li>图像编码器：ViT</li> <li>提示编码器：处理点、框等提示</li> <li>掩码解码器：Transformer 结构</li> </ul> </li> </ul> <hr/> <h4 id="-提示编码器"><strong>📌 提示编码器</strong></h4> <ol> <li><strong>标记点编码器</strong> <ul> <li>二维坐标 → 高维嵌入（与图像编码维度一致）</li> <li>利用可学习变量赋予语义信息</li> </ul> </li> <li><strong>标记框编码器</strong> <ul> <li>两个角点坐标转换为高维嵌入</li> <li>用可学习权重加权融合</li> </ul> </li> <li><strong>掩码编码器</strong> <ul> <li>卷积操作将输入掩码映射到与图像嵌入一致的维度</li> </ul> </li> </ol> <hr/> <h4 id="-sam-处理流程"><strong>📌 SAM 处理流程</strong></h4> <div style="text-align:center"> <img src="/assets/img/blogs/sam/process.png" alt="SAM处理流程" style="max-width:90%; height:auto;"/> </div> <ol> <li><strong>点 &amp; 框嵌入处理</strong> <ul> <li>定义 <code class="language-plaintext highlighter-rouge">iou_token</code> 和 <code class="language-plaintext highlighter-rouge">mask_tokens</code>（4个），与 sparse embeddings concat 形成 tokens</li> </ul> </li> <li><strong>跨模态交互数据构建</strong> <ul> <li>复制 image_embeddings，使每个 token 可访问完整图像特征</li> <li>与 dense embeddings 相加，并复制位置编码</li> </ul> </li> <li><strong>TwoWayTransformer</strong> <ul> <li>输入：image_embeddings、位置编码、tokens</li> <li>模块包含： <ul> <li>tokens 自注意力</li> <li>tokens(Q) → image(K) 交叉注意力</li> <li>MLP</li> <li>image(Q) → tokens(K) 交叉注意力</li> </ul> </li> <li>处理两轮后得到： <ul> <li>更新 tokens → <code class="language-plaintext highlighter-rouge">hs</code></li> <li>更新 image_embeddings → <code class="language-plaintext highlighter-rouge">src</code></li> </ul> </li> <li>每次 attention 都加位置编码，增强位置敏感性</li> </ul> </li> </ol> <div style="text-align:center"> <img src="/assets/img/blogs/sam/twoway.png" alt="TwoWayTransformer结构" style="max-width:85%; height:auto;"/> </div> <ol> <li><strong>预测 Mask</strong></li> </ol> <div style="text-align:center"> <img src="/assets/img/blogs/sam/predict.png" alt="SAM预测Mask" style="max-width:85%; height:auto;"/> </div> <ul> <li><code class="language-plaintext highlighter-rouge">hs</code> 拆解 <code class="language-plaintext highlighter-rouge">iou_token</code> 和 <code class="language-plaintext highlighter-rouge">mask_tokens</code> → 全连接调整尺寸</li> <li><code class="language-plaintext highlighter-rouge">src</code> → resize + 卷积上采样</li> <li>mask_tokens × src → 得到 4 个 mask（only one、whole、part、subpart）</li> <li>iou_token 表示 4 个 mask 的 IoU</li> </ul> <hr/> <h4 id="-dinov2-浅述"><strong>📌 DINOv2 浅述</strong></h4> <ul> <li><strong>自监督学习</strong>：无需人工标注，从海量图像中提取通用视觉特征</li> <li><strong>教师-学生机制</strong>： <ul> <li>教师网络看大局、定目标，提供稳定语义信息</li> <li>学生网络看局部、猜全局，逼近教师输出</li> </ul> </li> <li><strong>下游任务适用性</strong>：分类、分割、检测等均可直接使用输出特征</li> </ul> <hr/> <h4 id="-分割发展趋势"><strong>📌 分割发展趋势</strong></h4> <p>近年来图像分割呈现明显趋势：</p> <ol> <li><strong>Promptable 分割</strong> <ul> <li>通过点、框或文本提示控制分割目标</li> <li>分割任务更灵活、泛化能力更强</li> </ul> </li> <li><strong>大模型 vs 轻量化模型</strong> <ul> <li>大模型（CLIP、SAM）：数据量大、参数多，适合开放域和复杂环境，但推理成本高</li> <li>轻量化模型（BiSeNetV2、SegFormer）：结构紧凑、推理快，适合实时或资源受限场景</li> </ul> </li> <li><strong>多模态与交互分割</strong> <ul> <li>结合图像 + 文本/提示输入</li> <li>支持自然语言或交互式控制分割目标</li> <li>提升语义理解能力，更贴近实际需求</li> </ul> </li> </ol> <blockquote> <p>总结：SAM 等模型通过引入提示驱动、跨模态交互和大规模训练，实现了零样本、高泛化分割，未来图像分割的发展将更注重 <strong>多模态、可交互和开放域适应能力</strong>。</p> </blockquote> <hr/> <div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;"> <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p> <ul style="list-style:none; padding-left:0;"> <li>⬅️ 上一篇：<a href="/blog/2024/transformer-segment">基于Transformer的分割模型解析——SETR、Segmenter、SegFormer与MaskFormer</a></li> </ul> </div>]]></content><author><name></name></author><category term="研究笔记"/><category term="计算机视觉"/><category term="SAM"/><category term="图像分割"/><category term="多模态"/><category term="Transformer"/><category term="DINOv2"/><summary type="html"><![CDATA[SAM（Segment Anything Model）及其提示驱动分割机制解析，同时浅述 DINOv2 与未来分割趋势。]]></summary></entry><entry><title type="html">多模态视觉大模型解析——CLIP</title><link href="https://xqsrd666.github.io/blog/2024/clip/" rel="alternate" type="text/html" title="多模态视觉大模型解析——CLIP"/><published>2024-11-30T11:00:00+00:00</published><updated>2024-11-30T11:00:00+00:00</updated><id>https://xqsrd666.github.io/blog/2024/clip</id><content type="html" xml:base="https://xqsrd666.github.io/blog/2024/clip/"><![CDATA[<p>在前几篇文章中，我们介绍了 <strong>Transformer 系列在视觉任务中的应用</strong>，包括 ViT、SegFormer、MaskFormer 等模型。<br/> 这些模型侧重图像的全局建模与分割任务，但在实际应用中，往往还需要<strong>图像与文本的跨模态理解能力</strong>。</p> <p>为此，多模态视觉大模型应运而生，其中最具代表性的是 <strong>CLIP（Contrastive Language-Image Pre-training）</strong>。<br/> 它通过对比学习方法，将图像与文本映射到共享语义空间，实现零样本分类与跨模态检索等能力。</p> <hr/> <h4 id="-clipcontrastive-language-image-pre-training"><strong>📌 CLIP（Contrastive Language-Image Pre-training）</strong></h4> <div style="text-align:center"> <img src="/assets/img/blogs/clip/clip.png" alt="CLIP结构" style="max-width:85%; height:auto;"/> </div> <ul> <li><strong>目标</strong>：解决传统监督学习模型高度专业化、迁移能力差的问题</li> <li><strong>方法</strong>：通过大规模图像-文本对数据集进行对比学习</li> <li><strong>核心思想</strong>：将文本与图像编码到共享语义向量空间，使语义相近的图像与文本在空间中距离更近</li> </ul> <hr/> <h4 id="核心逻辑与组成部分"><strong>核心逻辑与组成部分</strong></h4> <ol> <li><strong>文本编码器</strong> <ul> <li>标准 Transformer Encoder</li> <li>输入文本序列 → 多头自注意力 → CLS token 表示整个文本语义</li> </ul> </li> <li><strong>图像编码器</strong> <ul> <li>可选 CNN（ResNet）或 Transformer（ViT）</li> <li>输出图像嵌入特征</li> </ul> </li> <li><strong>对比学习训练</strong> <ul> <li>正样本：匹配的图像-文本对</li> <li>负样本：非匹配的图像-文本对</li> <li>计算余弦相似度，最大化正样本距离，最小化负样本距离</li> </ul> </li> <li><strong>共享语义向量空间</strong> <ul> <li>训练完成后，图像和文本特征在同一向量空间内可直接比较</li> <li>支持跨模态检索、零样本分类等任务</li> </ul> </li> </ol> <hr/> <h4 id="-训练与-0-样本分类"><strong>📌 训练与 0 样本分类</strong></h4> <ul> <li><strong>训练流程</strong>： <ol> <li>对 N 个图像-文本对编码 → N 个正样本 + N²-N 个负样本</li> <li>构建相似度矩阵</li> <li>对比学习优化正负样本距离</li> </ol> </li> <li><strong>0 样本图像分类</strong>： <ol> <li>对数据集中的类别文本进行编码</li> <li>对输入图像进行编码</li> <li>计算图像向量与所有类别文本向量相似度</li> <li>相似度最高类别即为预测类别</li> </ol> </li> </ul> <p><strong>优势</strong>：</p> <ul> <li>高效训练、任务简单</li> <li>灵活迁移到新任务、细粒度类别</li> <li>全局信息学习，不局限单个物体</li> </ul> <hr/> <h4 id="-笔试题示例"><strong>📌 笔试题示例</strong></h4> <ol> <li> <p><strong>CLIP 模型核心机制与语义对齐原理</strong></p> <p><strong>答</strong>：CLIP 采用 <strong>双编码器结构</strong>（图像编码器 + 文本编码器）和 <strong>对比损失（InfoNCE）</strong></p> <p>核心步骤：</p> <ol> <li>将匹配的图像-文本对作为正样本，非匹配作为负样本</li> <li>批次内计算所有图像与文本特征的相似度矩阵</li> <li>对比学习拉近正样本的距离，推远负样本的距离</li> <li>最终实现图像与文本语义在特征空间中的对齐</li> </ol> </li> <li> <p><strong>无监督视频片段-文本语义匹配算法</strong></p> <p><strong>任务描述</strong>：</p> <ul> <li><strong>输入</strong>：视频 V={v0,v1,…,vm}（m帧），解说文本 T={t0,t1,…,tn}</li> <li><strong>输出</strong>：视频片段起始帧 vs 和结束帧 ve，使其与文本区间 Tij={ti,…,tj} 匹配</li> <li><strong>约束</strong>：无监督</li> </ul> <p><strong>算法步骤</strong>：</p> <ol> <li><strong>特征提取</strong> <ul> <li>用 CLIP 提取视频每帧与文本段 T_{ij} 的特征向量</li> </ul> </li> <li><strong>相似度计算</strong> <ul> <li>计算每帧与文本段的余弦相似度 → 得到相似度序列 S</li> </ul> </li> <li><strong>片段搜索</strong> <ul> <li>遍历所有可能的视频片段 [v_s, v_e]</li> <li>计算片段内相似度均值</li> </ul> </li> <li><strong>最优选择</strong> <ul> <li>选择均值最高的片段作为最佳匹配</li> </ul> </li> </ol> <p><strong>优化方向（当 m 很大时）</strong>：</p> <ul> <li>降采样处理减少帧数</li> <li>设置最大片段长度限制</li> <li>多尺度搜索策略</li> <li>使用动态规划加速搜索</li> </ul> </li> </ol> <hr/> <blockquote> <p><strong>一句话总结</strong>：CLIP 利用对比学习将图像与文本映射到共享语义空间，实现跨模态理解、零样本分类及视频-文本匹配，为多模态视觉大模型奠定基础。</p> </blockquote> <div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;"> <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p> <ul style="list-style:none; padding-left:0;"> <li>⬅️ 上一篇：<a href="/transformer-segment">基于Transformer的分割模型解析——SETR、Segmenter、SegFormer与MaskFormer</a></li> <li>➡️ 下一篇：<a href="/blog/2024/sam">多模态视觉大模型解析——SAM</a></li> </ul> </div>]]></content><author><name></name></author><category term="研究笔记"/><category term="计算机视觉"/><category term="图像分割"/><category term="CLIP"/><category term="多模态"/><category term="视觉模型"/><category term="对比学习"/><summary type="html"><![CDATA[多模态视觉大模型CLIP解析，包括模型结构、训练机制、0样本分类及无监督视频-文本匹配算法。]]></summary></entry><entry><title type="html">基于Transformer的分割模型解析——SETR、Segmenter、SegFormer与MaskFormer</title><link href="https://xqsrd666.github.io/blog/2024/transformer-segment/" rel="alternate" type="text/html" title="基于Transformer的分割模型解析——SETR、Segmenter、SegFormer与MaskFormer"/><published>2024-11-15T20:30:00+00:00</published><updated>2024-11-15T20:30:00+00:00</updated><id>https://xqsrd666.github.io/blog/2024/transformer-segment</id><content type="html" xml:base="https://xqsrd666.github.io/blog/2024/transformer-segment/"><![CDATA[<p>在前几篇文章中，我们介绍了 <strong>CNN 系列模型 → 高效轻量化分割 BiSeNetv2 → ViT 与 MAE</strong>。<br/> 它们分别在局部细节捕捉与全局建模能力上奠定了基础。</p> <p>然而在语义分割任务中，我们不仅需要全局上下文，还希望 <strong>恢复图像边界和多尺度信息</strong>。<br/> 为此，基于 Transformer 的分割模型应运而生。本文将梳理典型模型的设计逻辑与改进策略。</p> <hr/> <h4 id="-setrsegmentation-transformer"><strong>📌 SETR（SEgmentation TRansformer）</strong></h4> <p>以 ViT 为 backbone 的首个语义分割代表模型。<br/> 在 ViT 编码器基础上，SETR 提出三种解码器策略：</p> <div style="text-align:center"> <img src="/assets/img/blogs/transformer-segment/image-20250909173759174.png" alt="SETR结构" style="max-width:85%; height:auto;"/> </div> <ol> <li><strong>双线性插值</strong>：简单上采样恢复分辨率</li> <li><strong>渐进式上采样</strong>：交替卷积 + 2 倍上采样，避免噪声累积</li> <li><strong>多级特征融合（MLA）</strong>： <ul> <li>从 Transformer 不同层抽取 M 个特征</li> <li>1D → 2D 重整 → 三次卷积（通道数逐步减半）</li> <li>上采样 → 通道级拼接 → 双线性上采样到原图尺寸</li> </ul> </li> </ol> <hr/> <h4 id="-segmenter"><strong>📌 Segmenter</strong></h4> <div style="text-align:center"> <img src="/assets/img/blogs/transformer-segment/Segmenter.png" alt="Segmenter结构" style="max-width:85%; height:auto;"/> </div> <ul> <li>编码器同 SETR，使用 ViT</li> <li>解码器使用 <strong>注意力机制</strong>： <ul> <li>类别嵌入向量主动查询图像嵌入特征</li> <li>通过自注意力计算 Mask，每个类别在每个 Patch 的语义匹配分数</li> <li>上采样回原图尺寸，生成像素级预测</li> </ul> </li> </ul> <hr/> <h4 id="-segformer简单高效的-transformer-分割设计"><strong>📌 SegFormer：简单高效的 Transformer 分割设计</strong></h4> <div style="text-align:center"> <img src="/assets/img/blogs/transformer-segment/Segformer.png" alt="SegFormer结构" style="max-width:85%; height:auto;"/> </div> <ul> <li>层次化 Transformer 编码器输出 <strong>多尺度特征</strong></li> <li>MLP 解码器聚合不同层级信息</li> <li>改进 ViT 局限性： <ul> <li>支持多尺度特征，捕捉上下文信息</li> <li>更灵活的位置信息融合</li> </ul> </li> </ul> <hr/> <h4 id="-vit-局限性回顾"><strong>📌 ViT 局限性回顾</strong></h4> <ol> <li>Patch 固定 → 单一特征尺寸 → 缺乏多尺度表征，边界恢复不精确</li> <li>计算量大 → 自注意力平方级复杂度</li> <li>固定尺寸位置编码 → 输入灵活性差</li> </ol> <hr/> <h4 id="-segformer-编码器改进策略"><strong>📌 SegFormer 编码器改进策略</strong></h4> <ol> <li><strong>逐层下采样</strong>（Overlap Patch Merge） <ul> <li>保持 patch 周围局部连续性</li> <li>第一次 4 倍，下采样后每次 2 倍，最终缩减 32 倍</li> </ul> </li> <li><strong>Efficient Self-Attention</strong>： <ul> <li>对 K 特征添加缩放因子 R</li> <li>Resize 缩小维度后全连接恢复，降低计算</li> </ul> </li> <li><strong>Mix-FFN</strong>： <ul> <li>前馈网络中融合 3×3 卷积</li> <li>注入位置信息，提高空间感知能力</li> </ul> </li> </ol> <hr/> <h4 id="-segformer-解码器设计"><strong>📌 SegFormer 解码器设计</strong></h4> <ul> <li>四层不同尺度特征</li> <li>线性层映射到统一维度 D</li> <li>双线性插值至 H/4 × W/4</li> <li>通道级拼接 → 线性映射 → 输出类别维度</li> </ul> <hr/> <h4 id="-实例分割-maskformer"><strong>📌 实例分割 MaskFormer</strong></h4> <div style="text-align:center"> <img src="/assets/img/blogs/transformer-segment/MaskFormer.jpg" alt="MaskFormer结构" style="max-width:85%; height:auto;"/> </div> <ul> <li><strong>传统方法</strong>：像素级预测 → 连通区域聚类 → 掩码生成 <ul> <li>优势：结构简单，小物体敏感</li> <li>局限：依赖局部感受野，难以建模长程依赖</li> </ul> </li> <li><strong>MaskFormer</strong>：自顶向下掩码级推理 <ul> <li>通过 Transformer 注意力计算像素关联</li> <li>动态生成类别掩码 → 高精度实例分割</li> </ul> </li> </ul> <p><strong>处理流程</strong>：</p> <ol> <li>Image feature → 聚类 Mask</li> <li>类似 DETR 的 preset queries → MSA → Q 对齐 per-pixel embedding</li> <li>点积得到 pixel-level mask prediction</li> <li>联合优化：分类 + 掩码损失（IoU + 样本平衡）</li> </ol> <hr/> <h4 id="-mask2former"><strong>📌 Mask2Former</strong></h4> <div style="text-align:center"> <img src="/assets/img/blogs/transformer-segment/Mask2Former.png" alt="Mask2Former结构" style="max-width:85%; height:auto;"/> </div> <ul> <li>基础结构不变，Transformer decoder 优化三方面： <ol> <li><strong>Masked Cross-Attention</strong>：加入上一层掩码，关注重要区域</li> <li><strong>多尺度特征输入</strong>：加入可学习尺度嵌入</li> <li><strong>Self-Attention 放在 Cross-Attention 后</strong>，X0 查询特征可学习，删除 Dropout</li> </ol> </li> </ul> <blockquote> <p>这种设计优化了查询初始化，结合位置编码引导 Transformer 学习特征，提高实例分割精度和稳定性。</p> </blockquote> <hr/> <h4 id="-总结"><strong>📌 总结</strong></h4> <ul> <li>SETR：首个 ViT 分割尝试，多解码器设计</li> <li>Segmenter：类别向量主动查询图像特征，生成 Mask</li> <li>SegFormer：多尺度特征 + 高效编码器，解决 ViT 局限</li> <li>MaskFormer/Mask2Former：实例分割的新范式，自顶向下动态掩码生成</li> </ul> <blockquote> <p><strong>一句话记住</strong>：基于 Transformer 的分割模型从 ViT 出发，不断优化编码器和解码器结构，兼顾全局建模、多尺度特征和实例掩码预测，实现语义与实例分割的新高度。</p> </blockquote> <hr/> <div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;"> <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p> <ul style="list-style:none; padding-left:0;"> <li>⬅️ 上一篇：<a href="/blog/2024/transformer-backbone-vit">基于Transformer的视觉Backbone——ViT与MAE</a></li> <li>➡️ 下一篇：<a href="/blog/2024/clip">多模态视觉大模型解析——CLIP</a></li> </ul> </div>]]></content><author><name></name></author><category term="研究笔记"/><category term="计算机视觉"/><category term="Transformer"/><category term="图像分割"/><category term="ViT"/><category term="SegFormer"/><category term="MaskFormer"/><summary type="html"><![CDATA[从 ViT 发展而来的语义分割模型，包括 SETR、Segmenter、SegFormer，以及实例分割的 MaskFormer/Mask2Former，解析其编码器、解码器与核心机制。]]></summary></entry><entry><title type="html">基于Transformer的视觉Backbone——ViT与MAE</title><link href="https://xqsrd666.github.io/blog/2024/transformer-backbone-vit/" rel="alternate" type="text/html" title="基于Transformer的视觉Backbone——ViT与MAE"/><published>2024-10-30T20:30:00+00:00</published><updated>2024-10-30T20:30:00+00:00</updated><id>https://xqsrd666.github.io/blog/2024/transformer-backbone-vit</id><content type="html" xml:base="https://xqsrd666.github.io/blog/2024/transformer-backbone-vit/"><![CDATA[<p>在前几篇文章中，我们依次介绍了 <strong>CNN 系列语义分割模型</strong>（FCN → U-Net → DeepLab）以及 <strong>高效轻量化分割模型 BiSeNetv2</strong>。<br/> 它们在不同阶段提升了分割精度与效率，但全局建模能力仍有限。</p> <p>随着 <strong>Transformer</strong> 的引入，视觉模型可以突破卷积的局部性限制，通过自注意力机制直接捕捉图像中长程依赖。<br/> 在视觉领域，ViT 与 MAE 是两个典型的 Transformer Backbone 案例，本文将详细解析它们的工作原理与核心机制。</p> <hr/> <h4 id="-基于-transformer-的-cv-backbone--vit"><strong>📌 基于 Transformer 的 CV Backbone — ViT</strong></h4> <p>核心思想：<strong>将图像划分为小块（Patch）</strong>，每个小块视作“单词”，整个图像看作一个由 Patch 组成的序列，通过 Transformer 编码器进行处理。</p> <div style="text-align:center"> <img src="/assets/img/blogs/transformer-backbone-vit/image-20250909163833271.png" alt="ViT结构示意图" style="max-width:85%; height:auto;"/> </div> <p>主要操作流程如下：</p> <ol> <li><strong>图像切块与嵌入（Patch Embedding）</strong> <ul> <li>将输入图像切分为大小为 16×16 的 Patch，每个 Patch 展平并映射到特征维度 768</li> <li>增加一个 <strong>CLS Token</strong>，用于聚合图像全局特征</li> <li>得到序列维度为 <code class="language-plaintext highlighter-rouge">197 × 768</code>，并将 <strong>位置编码</strong> 与 Patch 特征相加</li> </ul> </li> <li><strong>Transformer 编码器（Encoder）</strong> <ul> <li>每层包括：归一化 → 多头自注意力 → 残差连接 → 归一化</li> <li><strong>多头自注意力</strong>：特征拆分到不同子空间，分别计算 QKV，输出拼接形成全局特征</li> </ul> </li> <li><strong>前馈网络 MLP（Feed Forward Network）</strong> <ul> <li>两层全连接：先放大 → ReLU 激活 → 再缩小</li> <li>放大操作增强特征表达能力，使非线性变换可以学习更复杂特征</li> </ul> </li> <li><strong>Transformer Block 堆叠</strong> <ul> <li>多个 Block 顺序堆叠，实现深层特征抽象与全局上下文建模</li> </ul> </li> <li><strong>线性分类层</strong> <ul> <li>使用 CLS Token 的最终表示进行分类</li> <li>对比平均池化：CLS Token 是主动、动态、有选择的全局聚合，平均池化被动、静态、无差别</li> </ul> </li> </ol> <hr/> <h4 id="-核心知识点"><strong>📌 核心知识点</strong></h4> <p><strong>为什么图像可以当作序列处理？</strong><br/> 自注意力机制本质是计算序列中任意两个元素之间的关系。<br/> 对于图像而言，远距离 Patch 之间的关系可能同样重要，自注意力可以建模全局依赖，赋予模型“一眼看全图”的能力。</p> <hr/> <h4 id="-vit-的局限性"><strong>📌 ViT 的局限性</strong></h4> <ol> <li><strong>Patch 固定大小</strong> <ul> <li>对不同分辨率图像或分割任务不够灵活</li> <li>单一特征尺寸 → 缺乏多尺度表征 → 难以精准恢复细节边界</li> <li>Patch 数量不一致 → 需要插值位置编码 → 精度损失</li> </ul> </li> <li><strong>计算量较大</strong> <ul> <li>自注意力计算复杂度为 $O(N^2)$，随着 Patch 数量增加呈平方级增长</li> </ul> </li> </ol> <hr/> <h4 id="-基于-transformer-的-cv-backbone--mae"><strong>📌 基于 Transformer 的 CV Backbone — MAE</strong></h4> <p>MAE（Masked Autoencoder）在 ViT 基础上引入<strong>随机遮盖机制</strong>：</p> <ol> <li>操作流程与 ViT 相同，但随机遮盖部分 Patch（例如 75%）</li> <li>剩余 Patch 经过 Transformer 编码器处理</li> <li>输出维度为 <code class="language-plaintext highlighter-rouge">(196 * 0.25 + 1) * 768 ≈ 50 × 768</code></li> <li>编码器学会从少量可见 Patch 重建原图，增强表征能力</li> </ol> <p>这种遮盖策略有效提高了 <strong>数据效率</strong>，同时降低计算开销，尤其适合大规模预训练。</p> <hr/> <h4 id="-总结"><strong>📌 总结</strong></h4> <ul> <li><strong>ViT</strong>：将图像切块为序列，通过 Transformer 捕捉全局依赖</li> <li><strong>MAE</strong>：在 ViT 上增加随机遮盖，提高特征学习效率与泛化能力</li> <li><strong>核心优势</strong>：全局建模能力强，能够捕捉长程依赖</li> <li><strong>局限</strong>：Patch 尺寸固定、计算复杂度高，需要多尺度和效率优化</li> </ul> <blockquote> <p><strong>一句话记住</strong>：ViT 让视觉模型可以“看全局”，MAE 让模型在部分信息下仍能高效学习全局特征。</p> </blockquote> <hr/> <div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;"> <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p> <ul style="list-style:none; padding-left:0;"> <li>⬅️ 上一篇：<a href="/blog/2024/transformer-intro">Transformer架构浅述</a></li> <li>➡️ 下一篇：<a href="/blog/2024/transformer-segment">基于Transformer的分割模型解析——SETR、Segmenter、SegFormer与MaskFormer</a></li> </ul> </div>]]></content><author><name></name></author><category term="研究笔记"/><category term="计算机视觉"/><category term="图像分割"/><category term="图像处理"/><category term="Transformer"/><category term="ViT"/><category term="MAE"/><category term="CV Backbone"/><summary type="html"><![CDATA[在 CNN → 高效轻量化分割 → Transformer 的发展脉络下，ViT与MAE为视觉任务提供了全局建模能力，并为后续分割、识别任务奠定基础。]]></summary></entry><entry><title type="html">Transformer架构浅述</title><link href="https://xqsrd666.github.io/blog/2024/transformer-intro/" rel="alternate" type="text/html" title="Transformer架构浅述"/><published>2024-10-20T20:30:00+00:00</published><updated>2024-10-20T20:30:00+00:00</updated><id>https://xqsrd666.github.io/blog/2024/transformer-intro</id><content type="html" xml:base="https://xqsrd666.github.io/blog/2024/transformer-intro/"><![CDATA[<p>在前几篇文章中，我们介绍了 <strong>CNN 主导的语义分割模型</strong>（FCN → U-Net → DeepLab），以及 <strong>高效轻量化分割模型 BiSeNetv2</strong>。<br/> 这些方法在各自阶段推动了分割任务的发展，但仍存在一个核心局限：<strong>全局建模能力不足</strong>。</p> <p>CNN 的感受野需要通过逐层堆叠卷积才能扩大，本质仍是<strong>局部逐级建模</strong>；即便引入空洞卷积或金字塔池化，也只能近似解决全局依赖问题。<br/> 而在语义分割中，很多场景需要理解图像整体的布局与长程关系，例如“草地上的牛”和“海上的船”，单纯依赖卷积往往力不从心。</p> <p>👉 在这样的背景下，<strong>Transformer</strong> 进入了视觉领域。<br/> 它以 <strong>自注意力机制</strong> 为核心，使序列中任意两个元素可以直接交互，天然具备全局建模能力。本文将简要介绍 Transformer 的核心架构与原理，为后续的 <strong>基于 Transformer 的分割模型（如 ViT、SegFormer）</strong> 打下基础。</p> <hr/> <h4 id="-transformer整体架构"><strong>📌 Transformer整体架构</strong></h4> <p>Transformer 最早由 [Attention is All You Need, 2017] 提出，用于机器翻译任务。<br/> 它完全抛弃了 RNN 和 CNN，直接基于 <strong>自注意力机制（Self-Attention）</strong> 建模序列中任意位置的依赖关系。</p> <p>整体结构由 <strong>编码器（Encoder）</strong> 和 <strong>解码器（Decoder）</strong> 组成：</p> <div style="text-align:center"> <img src="/assets/img/blogs/transformer-intro/Transformer_full_architecture.png" alt="Transformer整体架构" style="max-width:90%; height:auto;"/> </div> <hr/> <h5 id="1-编码器encoder"><strong>1. 编码器（Encoder）</strong></h5> <p>输入序列（如一句话的单词或图像划分的 patch）经过 <strong>嵌入（Embedding）</strong> 和 <strong>位置编码（Positional Encoding）</strong>，送入多层编码器堆叠。</p> <p>每层编码器包含两个核心模块：</p> <ol> <li><strong>多头自注意力（Multi-Head Self-Attention, MSA）</strong></li> <li><strong>前馈神经网络（Feed Forward Network, FFN）</strong></li> </ol> <p>编码器的作用是：<strong>在序列内部进行全局信息交互，得到更抽象、更语义化的表示</strong>。</p> <hr/> <h5 id="2-解码器decoder"><strong>2. 解码器（Decoder）</strong></h5> <p>解码器在自然语言生成中用于逐步生成目标序列。结构类似编码器，但增加了两次注意力机制：</p> <ol> <li><strong>Masked Multi-Head Attention</strong>：保证当前时刻只能看到已生成的内容。</li> <li><strong>Cross-Attention</strong>：使用解码器状态作为 Query，编码器输出作为 Key/Value，实现“查询源信息”。</li> </ol> <p>在视觉任务中（如分割），通常只保留 <strong>编码器部分</strong>，直接输出像素级预测，不需要解码器。</p> <hr/> <h4 id="-自注意力机制self-attention"><strong>📌 自注意力机制（Self-Attention）</strong></h4> <p>Transformer 核心是自注意力机制，每个元素都可以与序列内任意其他元素直接交互。</p> <p>输入序列 $X$ 经过线性映射得到 <strong>查询向量 Q、键向量 K、值向量 V</strong>：</p> \[Q = X W^Q, \quad K = X W^K, \quad V = X W^V\] <p>注意力分数通过点积计算：</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V\] <p>直观理解：</p> <ul> <li>每个元素会计算与所有元素的相关性；</li> <li>根据相关性加权求和，得到融合全局信息的新表示。</li> </ul> <hr/> <h4 id="-多头注意力multi-head-attention"><strong>📌 多头注意力（Multi-Head Attention）</strong></h4> <p>单一注意力机制可能只关注一种关系，而复杂任务需要考虑多维依赖。<br/> 于是 Transformer 使用 <strong>多头注意力</strong>：</p> <ul> <li>将输入拆分为多个子空间（子维度）；</li> <li>在每个子空间内独立计算注意力；</li> <li>最后拼接所有子结果。</li> </ul> <div style="text-align:center"> <img src="/assets/img/blogs/transformer-intro/image-20250908170117006.png" alt="多头注意力示意图" style="max-width:85%; height:auto;"/> </div> <p>这样可以让模型从不同角度捕捉依赖关系，提升表达能力。</p> <hr/> <h4 id="-位置编码positional-encoding"><strong>📌 位置编码（Positional Encoding）</strong></h4> <p>与 CNN 不同，Transformer <strong>没有内置顺序感</strong>，因此需要显式加入位置信息：</p> <ul> <li><strong>固定式</strong>：利用正弦/余弦函数生成位置向量</li> <li><strong>可学习式</strong>：将位置向量作为参数随训练更新</li> </ul> <p>在视觉任务中，常将图像切分为 patch，再加位置编码以保持空间结构。</p> <hr/> <h4 id="-残差连接与前馈网络"><strong>📌 残差连接与前馈网络</strong></h4> <p>每个注意力层和前馈层都使用 <strong>残差连接（Residual Connection） + 层归一化（LayerNorm）</strong>，防止梯度消失并加快收敛。</p> <p>前馈网络通常为两个全连接层，中间带激活函数（如 ReLU、GELU），用于增强特征表达能力。</p> <hr/> <h4 id="-transformer-与-cnn-对比"><strong>📌 Transformer 与 CNN 对比</strong></h4> <p><strong>CNN</strong>：</p> <ul> <li>核心：局部卷积 + 权值共享</li> <li>特点：空间局部性 + 平移不变性（归纳偏置强）</li> <li>优势：参数高效，计算快，擅长捕获局部细节</li> <li>局限：全局建模能力弱，需要逐层堆叠扩大感受野</li> </ul> <p><strong>Transformer</strong>：</p> <ul> <li>核心：自注意力机制</li> <li>特点：任意两个位置直接交互（归纳偏置弱）</li> <li>优势：擅长长程依赖、复杂语义关联</li> <li>局限：计算复杂度高，需要大量数据</li> </ul> <p><strong>总结</strong>：两者互补。现代视觉模型趋势是：</p> <ul> <li>CNN 融入注意力机制 → 增强全局感知</li> <li>Transformer 融入卷积 → 降低计算开销</li> </ul> <hr/> <h4 id="-总结"><strong>📌 总结</strong></h4> <p>Transformer 的引入，让语义分割从 <strong>局部逐层建模（CNN）</strong> 进入了 <strong>全局直接建模（Attention）</strong> 的新阶段：</p> <ul> <li><strong>自注意力机制</strong>：实现任意元素交互，打破感受野限制</li> <li><strong>多头注意力 + 位置编码</strong>：捕捉多维关系，保持空间结构</li> <li><strong>视觉应用</strong>：为 SegFormer、Mask2Former、多模态分割模型打下基础</li> </ul> <blockquote> <p><strong>一句话记住</strong>：CNN 擅长“局部细节”，Transformer 擅长“全局依赖”，两者结合是视觉分割未来发展方向。</p> </blockquote> <hr/> <div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;"> <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p> <ul style="list-style:none; padding-left:0;"> <li>⬅️ 上一篇：<a href="/blog/2024/bisenetv2">高效语义分割——BiSeNetv2的网络设计与训练策略</a></li> <li>➡️ 下一篇：<a href="/blog/2024/transformer-backbone-vit">基于Transformer的视觉Backbone——ViT与MAE</a></li> </ul> </div>]]></content><author><name></name></author><category term="研究笔记"/><category term="图像分割"/><category term="图像处理与视觉算法"/><category term="Transformer"/><category term="计算机视觉"/><summary type="html"><![CDATA[Transformer 以自注意力机制为核心，突破了卷积的局部性限制，能够直接建模全局依赖。在语义分割的发展脉络中，它代表着从高效CNN迈向全局建模的新阶段。]]></summary></entry><entry><title type="html">高效语义分割——BiSeNetv2的网络设计与训练策略</title><link href="https://xqsrd666.github.io/blog/2024/bisenetv2/" rel="alternate" type="text/html" title="高效语义分割——BiSeNetv2的网络设计与训练策略"/><published>2024-10-09T20:30:00+00:00</published><updated>2024-10-09T20:30:00+00:00</updated><id>https://xqsrd666.github.io/blog/2024/bisenetv2</id><content type="html" xml:base="https://xqsrd666.github.io/blog/2024/bisenetv2/"><![CDATA[<p>在前两篇文章中，我们梳理了从 <strong>FCN</strong> 到 <strong>DeepLab 系列</strong> 的演化过程。<br/> 这些方法显著提升了语义分割的精度，尤其是 DeepLab 系列通过空洞卷积和多尺度上下文有效增强了全局建模能力。</p> <p>然而，这类模型普遍存在一个现实问题：<strong>计算量大、推理速度慢</strong>。在自动驾驶、视频监控、AR/VR 等需要实时响应的场景中，它们往往难以直接落地。<br/> 因此，研究的重心逐渐转向了 <strong>高效轻量化分割模型</strong> —— 如何在 <strong>保证精度</strong> 的前提下，实现 <strong>更快的推理速度</strong>。</p> <hr/> <h4 id="-背景介绍"><strong>📌 背景介绍</strong></h4> <p>为了加速分割模型的训练与推理，常见方法包括：</p> <ul> <li><strong>裁剪或 resize 限定输入大小</strong>：会损失细节，导致精度下降。</li> <li><strong>减少网络通道数量</strong>：降低特征提取能力，弱化空间表达。</li> <li><strong>丢弃最后阶段下采样</strong>（如 ENet）：特征尺寸较大 → 感受野不足 → 大物体识别困难。</li> </ul> <p><strong>总结</strong>：这些方法都依赖于 <strong>丢失细节</strong> 或 <strong>牺牲空间表达能力</strong> 来换取效率。</p> <p>一些改进措施（如 U-shape）利用跳跃连接在上采样时恢复细节，但计算开销较大，且下采样丢失的信息无法完全逆转。</p> <hr/> <h4 id="-bisenetv2-网络架构"><strong>📌 BiSeNetv2 网络架构</strong></h4> <p>核心思想：<strong>双分支结构</strong> ——</p> <ul> <li><strong>细节分支</strong>：保留高分辨率的细节特征。</li> <li><strong>语义分支</strong>：快速下采样，提取全局语义特征。</li> <li><strong>BGALayer</strong>：引导聚合层，融合两类特征。</li> <li><strong>SegmentHead</strong>：辅助训练，提高精度而不增加推理开销。</li> </ul> <div style="text-align:center"> <img src="/assets/img/blogs/bisenetv2/image-20250905194552447.png" alt="BiSeNetv2整体结构" style="max-width:90%; height:auto;"/> </div> <hr/> <h4 id="1-细节分支detail-branch"><strong>1. 细节分支（Detail Branch）</strong></h4> <p>三层卷积-归一化-激活结构：</p> <ul> <li>输入通道数 3 → 128</li> <li>分辨率缩小至 1/8</li> </ul> <p>👉 负责捕获图像的低级细节与高分辨率特征。</p> <hr/> <h4 id="2-语义分支semantic-branch"><strong>2. 语义分支（Semantic Branch）</strong></h4> <p>分辨率逐步缩小至 1/32，通道数 3 → 128。<br/> 包含以下关键模块：</p> <p><strong>(1) StemBlock：高效初始下采样</strong></p> <ul> <li>左右双路径： <ul> <li>左：卷积下采样，学习抽象特征</li> <li>右：最大池化，保留显著特征</li> </ul> </li> <li>拼接后得到平衡抽象与细节的特征</li> </ul> <div style="text-align:center"> <img src="/assets/img/blogs/bisenetv2/image-20250612112210233.png" alt="StemBlock结构" style="max-width:80%; height:auto;"/> </div> <hr/> <p><strong>(2) GELayerS2：带残差的 2 倍下采样卷积层</strong></p> <ul> <li>主路径：预卷积 → 深度下采样 → 增强 → 压缩</li> <li>残差路径：深度下采样 → 通道调整</li> <li>融合后既保证信息压缩，又避免原始信息丢失</li> </ul> <div style="text-align:center"> <img src="/assets/img/blogs/bisenetv2/image-20250612115615598.png" alt="GELayerS2结构" style="max-width:80%; height:auto;"/> </div> <hr/> <p><strong>(3) GELayerS1：不改变分辨率的特征增强层</strong></p> <ul> <li>主路径：卷积 → 深度扩展 → 压缩</li> <li>残差路径：原输入</li> <li>融合后增强特征表达</li> </ul> <div style="text-align:center"> <img src="/assets/img/blogs/bisenetv2/image-20250612115359138.png" alt="GELayerS1结构" style="max-width:80%; height:auto;"/> </div> <hr/> <p><strong>(4) CEBlock：全局上下文建模</strong></p> <ul> <li>全局平均池化 (GAP) 提取通道全局特征</li> <li>标准化 &amp; 卷积增强</li> <li>将全局特征广播回每个位置，实现 <strong>全局-局部融合</strong></li> </ul> <div style="text-align:center"> <img src="/assets/img/blogs/bisenetv2/image-20250612114432081.png" alt="CEBlock结构" style="max-width:80%; height:auto;"/> </div> <hr/> <h4 id="3-bgalayer双分支融合"><strong>3. BGALayer：双分支融合</strong></h4> <p>利用 <strong>语义分支主导的自注意力门控机制</strong>，引导细节分支：</p> <ul> <li>抑制无关区域</li> <li>强化目标区域</li> <li>保持高分辨率细节</li> </ul> <div style="text-align:center"> <img src="/assets/img/blogs/bisenetv2/image-20250612162908983.png" alt="BGALayer结构" style="max-width:85%; height:auto;"/> </div> <hr/> <h4 id="4-segmenthead辅助训练与推理"><strong>4. SegmentHead：辅助训练与推理</strong></h4> <ul> <li>多级放大（不同尺度上采样）</li> <li>特征精修，边界优化</li> <li>类别校准，像素级预测</li> <li>训练阶段：增加辅助监督信号，加速收敛</li> <li>推理阶段：可丢弃，不增加额外开销</li> </ul> <hr/> <h4 id="-总结"><strong>📌 总结</strong></h4> <p>BiSeNetv2 设计要点：</p> <ol> <li><strong>双分支结构</strong>：细节分支捕获空间信息，语义分支获取全局语境。</li> <li><strong>BGALayer</strong>：实现高效的跨分支特征融合。</li> <li><strong>辅助训练策略</strong>：提升精度而不增加推理开销。</li> </ol> <p>相比以往单一路径网络，BiSeNetv2 彻底解决了 <strong>“既要细节又要全局”</strong> 的矛盾，在速度和精度上实现平衡。</p> <p>👉 这也是为什么 BiSeNetv2 能成为实时语义分割任务的代表性方法。</p> <hr/> <h4 id="-权重初始化与训练流程"><strong>📌 权重初始化与训练流程</strong></h4> <ul> <li><strong>权重初始化</strong> <ul> <li>卷积/线性层：<code class="language-plaintext highlighter-rouge">N(0, √(2/fan_out))</code> 随机采样，偏置清零</li> <li>BN 层：普通 BN (γ=1, β=0)，最后一层 BN (γ=0, β=0)</li> </ul> </li> <li><strong>训练流程</strong> <ol> <li>数据送入 GPU</li> <li>清空梯度</li> <li><strong>自动混合精度训练 (AMP)</strong> <ul> <li>FP16：卷积、矩阵乘法（高效低精度）</li> <li>FP32：归约操作、Loss 计算（稳定高精度）</li> </ul> </li> <li>计算主损失 + 辅助损失</li> <li>反向传播与参数更新</li> </ol> </li> </ul> <hr/> <h4 id="-结语"><strong>📌 结语</strong></h4> <p>BiSeNetv2 在语义分割领域提出了一个兼顾速度与精度的解决方案，特别适合 <strong>交通、自动驾驶等实时场景</strong>。<br/> 它不仅提供了强大的特征建模能力，还启发我们在网络设计中如何平衡 <strong>局部细节与全局语义</strong>。</p> <blockquote> <p><strong>一句话总结</strong>：BiSeNetv2 的设计逻辑就是——细节分支保真，语义分支提纲，两者在 BGALayer 中高效融合。</p> </blockquote> <hr/> <div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;"> <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p> <ul style="list-style:none; padding-left:0;"> <li>⬅️ 上一篇：<a href="/blog/2024/development-cnn-segment">语义分割发展历程——FCNN、SegNet、U-Net、PSPNet与DeepLab</a></li> <li>➡️ 下一篇：<a href="/blog/2024/segformer">Transformer架构浅述</a></li> </ul> </div>]]></content><author><name></name></author><category term="项目实践"/><category term="图像分割"/><category term="图像处理与视觉算法"/><category term="计算机视觉"/><summary type="html"><![CDATA[BiSeNetv2 通过双分支结构同时保留空间细节与全局语义信息，实现轻量高效的语义分割，兼顾速度与精度。]]></summary></entry></feed>