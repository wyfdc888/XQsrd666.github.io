---
layout: post
title: 多模态视觉大模型解析——SAM
date: 2024-12-15 11:00:00
description: SAM（Segment Anything Model）及其提示驱动分割机制解析，同时浅述 DINOv2 与未来分割趋势。
thumbnail: /assets/img/blogs/sam/cover.jpg
tags: [计算机视觉, SAM, 图像分割, 多模态, Transformer, DINOv2]
categories: 研究笔记
---

在前几篇文章中，我们介绍了 **基于 Transformer 的语义分割模型（SETR、Segmenter、SegFormer、MaskFormer）**。  
随着模型能力提升和大规模数据可用，研究逐渐从固定任务分割向 **零样本、可交互、多模态分割** 发展。  
SAM（Segment Anything Model）即是这一趋势的代表，具备 **零样本泛化能力**，通过灵活的提示（点、框、文字）即可分割特定对象。

---

#### **📌 SAM（Segment Anything Model）概览**

<div style="text-align:center">
  <img src="/assets/img/blogs/sam/sam.png" alt="SAM模型" style="max-width:85%; height:auto;">
</div>

- **核心目标**：通过用户提示从图像中分割特定对象  
- **零样本泛化**：无需任务特定标注即可进行分割  
- **训练数据**：基于 SA-1B 大规模图像分割数据集  
- **主要模块**：
  - 图像编码器：ViT  
  - 提示编码器：处理点、框等提示  
  - 掩码解码器：Transformer 结构  

---

#### **📌 提示编码器**

1. **标记点编码器**  
   - 二维坐标 → 高维嵌入（与图像编码维度一致）  
   - 利用可学习变量赋予语义信息  

2. **标记框编码器**  
   - 两个角点坐标转换为高维嵌入  
   - 用可学习权重加权融合  

3. **掩码编码器**  
   - 卷积操作将输入掩码映射到与图像嵌入一致的维度  

---

#### **📌 SAM 处理流程**

<div style="text-align:center">
  <img src="/assets/img/blogs/sam/process.png" alt="SAM处理流程" style="max-width:90%; height:auto;">
</div>

1. **点 & 框嵌入处理**  
   - 定义 `iou_token` 和 `mask_tokens`（4个），与 sparse embeddings concat 形成 tokens  

2. **跨模态交互数据构建**  
   - 复制 image_embeddings，使每个 token 可访问完整图像特征  
   - 与 dense embeddings 相加，并复制位置编码  

3. **TwoWayTransformer**  
   - 输入：image_embeddings、位置编码、tokens  
   - 模块包含：
     - tokens 自注意力  
     - tokens(Q) → image(K) 交叉注意力  
     - MLP  
     - image(Q) → tokens(K) 交叉注意力  
   - 处理两轮后得到：
     - 更新 tokens → `hs`  
     - 更新 image_embeddings → `src`  
   - 每次 attention 都加位置编码，增强位置敏感性  

<div style="text-align:center">
  <img src="/assets/img/blogs/sam/twoway.png" alt="TwoWayTransformer结构" style="max-width:85%; height:auto;">
</div>

4. **预测 Mask**  

<div style="text-align:center">
  <img src="/assets/img/blogs/sam/predict.png" alt="SAM预测Mask" style="max-width:85%; height:auto;">
</div>

- `hs` 拆解 `iou_token` 和 `mask_tokens` → 全连接调整尺寸  
- `src` → resize + 卷积上采样  
- mask_tokens × src → 得到 4 个 mask（only one、whole、part、subpart）  
- iou_token 表示 4 个 mask 的 IoU  

---

#### **📌 DINOv2 浅述**

- **自监督学习**：无需人工标注，从海量图像中提取通用视觉特征  
- **教师-学生机制**：
  - 教师网络看大局、定目标，提供稳定语义信息  
  - 学生网络看局部、猜全局，逼近教师输出  
- **下游任务适用性**：分类、分割、检测等均可直接使用输出特征  

---

#### **📌 分割发展趋势**

近年来图像分割呈现明显趋势：

1. **Promptable 分割**  
   - 通过点、框或文本提示控制分割目标  
   - 分割任务更灵活、泛化能力更强  

2. **大模型 vs 轻量化模型**  
   - 大模型（CLIP、SAM）：数据量大、参数多，适合开放域和复杂环境，但推理成本高  
   - 轻量化模型（BiSeNetV2、SegFormer）：结构紧凑、推理快，适合实时或资源受限场景  

3. **多模态与交互分割**  
   - 结合图像 + 文本/提示输入  
   - 支持自然语言或交互式控制分割目标  
   - 提升语义理解能力，更贴近实际需求  

> 总结：SAM 等模型通过引入提示驱动、跨模态交互和大规模训练，实现了零样本、高泛化分割，未来图像分割的发展将更注重 **多模态、可交互和开放域适应能力**。

---

<div class="post-navigation" style="margin-top:2rem; padding-top:1rem; border-top:1px solid #eaeaea;">
  <p style="margin-bottom:0.5rem; font-weight:600;">📖 系列文章导航</p>
  <ul style="list-style:none; padding-left:0;">
    <li>⬅️ 上一篇：<a href="{{ '/blog/2024/transformer-segment' | relative_url }}">基于Transformer的分割模型解析——SETR、Segmenter、SegFormer与MaskFormer</a></li>
  </ul>
</div>